from keras.callbacks import ModelCheckpoint, Callback, TensorBoard

import glob, re, os, sys

sys.path.insert(1, os.path.join(sys.path[0], '../../'))

from utils import divider
from constants import WEIGHTS_BASE_DIR, WEIGHTS_EXTENSION


def train_model_no_augment(model, train_data, train_labels, val_data, val_labels, batch_size, epochs, model_name,
                           class_weights=None, verbose=1, tb=False):
    """
    Training wrappper. Trains a given model with given params. Can be set to augment if needed. Checkpoints the best
    weights, and cleans them up when finished training.

    :param tb: TensorBoard enabled
    :param model: The model being trained
    :param train_data: Training data
    :param train_labels: Training labels
    :param val_data: Validation data. If none, 'val_split' times of the training data will be used as validation data
    :param batch_size: Batch size used when training
    :param epochs: Epochs to train
    :param augment: Augmentation object
    :param model_name: Name of the model. Model will be saved with this name
    :param verbose: Level of output
    :return:
    """
    callback_array = callbacks(model_name=model_name, checkpoint=True, delete_checkpoints=True, tb=tb)

    # Fit the model according to params
    model.fit(train_data, train_labels, batch_size=batch_size, epochs=epochs, verbose=verbose,
              validation_data=(val_data, val_labels), callbacks=callback_array, class_weight=class_weights)


def train_model_augment(model, train_data, train_labels, val_data, val_labels, batch_size, epochs, model_name,
                        augment, augment_batch_size=32, augment_validation=False, class_weights=None,
                        verbose=1, tb=False):
    # Get callbacks
    callback_array = callbacks(model_name=model_name, checkpoint=True, delete_checkpoints=True, tb=tb)

    # Set the augmentation and append it to the training data
    train_datagen = augment
    train_datagen.fit(train_data)

    if augment_validation:
        val_datagen = augment
        val_datagen.fit(val_data)
        val_datagen.flow(val_data, val_labels, batch_size=augment_batch_size)

    validation = val_datagen if augment_validation else (val_data, val_labels)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(generator=train_datagen.flow(train_data, train_labels, batch_size=augment_batch_size),
                        epochs=epochs, steps_per_epoch=len(train_data) // batch_size,
                        validation_data=validation, class_weight=class_weights, callbacks=callback_array,
                        verbose=verbose, validation_steps=len(train_data) // batch_size)


def callbacks(model_name, checkpoint=True, delete_checkpoints=True, tb=False):
    callback_array = []

    # Callback to save the best model
    checkpoint_callback = ModelCheckpoint(WEIGHTS_BASE_DIR + model_name + "_{epoch:03d}" + WEIGHTS_EXTENSION,
                                          verbose=1, save_best_only=True, monitor="val_loss")

    # Callback to cleanup the checkpoints
    delete_checkpoint = CheckpointCleanup(model_name)

    # TensorBoard callback
    tensorboard = TensorBoard(log_dir=f'logs/{model_name}')

    if checkpoint:
        callback_array.append(checkpoint_callback)
    if delete_checkpoints:
        callback_array.append(delete_checkpoint)
    if tb:
        callback_array.append(tensorboard)

    return callback_array


class CheckpointCleanup(Callback):
    """
    Custom callback to cleanup all excess checkpoint. Primarily used on the end of a training
    """

    def __init__(self, model_name):
        super(CheckpointCleanup, self).__init__()
        self.model_name = model_name

    def on_train_end(self, batch, logs={}):
        divider(70)
        print("Deleting all excess checkpoints")
        files = [file for file in glob.glob(WEIGHTS_BASE_DIR + self.model_name + '*' + WEIGHTS_EXTENSION)]
        numbs = []
        for file in files:
            try:
                found_num = re.search("_\d{3}", file).group().replace("_", "")
                numbs.append(int(found_num))
            except AttributeError:
                pass

        max_numb = max(numbs) if len(numbs) > 0 else 0
        max_numb_str = f"{str(max_numb).zfill(3)}"
        print(f"Best checkpoint was in epoch {max_numb_str}")
        for file in files:
            if max_numb_str in file:
                os.rename(file, WEIGHTS_BASE_DIR + self.model_name + WEIGHTS_EXTENSION)
            else:
                os.remove(file)
